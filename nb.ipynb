{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from torchvision import transforms, datasets, models\n",
    "import pytorch_lightning as pl\n",
    "from torchmetrics import Accuracy\n",
    "from random import shuffle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LightningDataModule\n",
    "@dataclass\n",
    "class SUN397DataModule(pl.LightningDataModule):\n",
    "    data_dir = \"./data\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.CenterCrop((32, 32)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    split = {\"val\": 10, \"test\": 40}\n",
    "    batch_size = 128\n",
    "\n",
    "    @property\n",
    "    def num_classes(self)->int:\n",
    "        return len(self.dataset.classes)\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # Download or preprocess data if required\n",
    "        _ = datasets.SUN397(root=self.data_dir, download=True)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Load and split the dataset into train and validation sets\n",
    "        self.dataset = datasets.SUN397(root=self.data_dir, transform=self.transform)\n",
    "        self.dataloader = partial(DataLoader, \n",
    "            dataset=self.dataset, batch_size=self.batch_size, \n",
    "            num_workers=torch.get_num_threads(), pin_memory=True,\n",
    "        )\n",
    "        self.cls_idxs = [[] for _ in self.dataset.classes]\n",
    "        for i, label in tqdm(enumerate(self.dataset._labels)):\n",
    "            self.cls_idxs[label].append(i)\n",
    "\n",
    "        for m in (\"val\", \"test\"):\n",
    "            setattr(self, f\"{m}_idxs\", [])\n",
    "        \n",
    "        for idx_list in tqdm(self.cls_idxs):\n",
    "            shuffle(idx_list)\n",
    "            self.val_idxs.extend(idx_list[:self.split[\"val\"]])\n",
    "            self.test_idxs.extend(idx_list[self.split[\"val\"]:sum(self.split.values())])\n",
    "        \n",
    "        self.train_idxs = list(set(range(len(self.dataset))) - set(self.val_idxs) - set(self.test_idxs))\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.dataloader(sampler=SubsetRandomSampler(self.train_idxs))\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.dataloader(sampler=SubsetRandomSampler(self.val_idxs))\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self.dataloader(sampler=SubsetRandomSampler(self.test_idxs))\n",
    "    \n",
    "dm = SUN397DataModule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "108754it [00:00, 3452026.56it/s]\n",
      "100%|██████████| 397/397 [00:00<00:00, 13426.37it/s]\n"
     ]
    }
   ],
   "source": [
    "class PreActResNetBlock(nn.Module):\n",
    "    def __init__(self, c_in, act_fn, subsample=False, c_out=-1):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            c_in - Number of input features\n",
    "            act_fn - Activation class constructor (e.g. nn.ReLU)\n",
    "            subsample - If True, we want to apply a stride inside the block and reduce the output shape by 2 in height and width\n",
    "            c_out - Number of output features. Note that this is only relevant if subsample is True, as otherwise, c_out = c_in\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if not subsample:\n",
    "            c_out = c_in\n",
    "\n",
    "        # Network representing F\n",
    "        self.net = nn.Sequential(\n",
    "            nn.BatchNorm2d(c_in),\n",
    "            act_fn(),\n",
    "            nn.Conv2d(c_in, c_out, kernel_size=3, padding=1, stride=1 if not subsample else 2, bias=False),\n",
    "            nn.BatchNorm2d(c_out),\n",
    "            act_fn(),\n",
    "            nn.Conv2d(c_out, c_out, kernel_size=3, padding=1, bias=False),\n",
    "        )\n",
    "\n",
    "        # 1x1 convolution needs to apply non-linearity as well as not done on skip connection\n",
    "        self.downsample = (\n",
    "            nn.Sequential(nn.BatchNorm2d(c_in), act_fn(), nn.Conv2d(c_in, c_out, kernel_size=1, stride=2, bias=False))\n",
    "            if subsample\n",
    "            else None\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.net(x)\n",
    "        if self.downsample is not None:\n",
    "            x = self.downsample(x)\n",
    "        out = z + x\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/DATA1/souraviai/M2m/nb.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B10.50.20.10/DATA1/souraviai/M2m/nb.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m dm\u001b[39m.\u001b[39;49mnum_classes()\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes=10,\n",
    "        num_blocks=[2, 2, 2, 2],\n",
    "        c_hidden=[64, 128, 256, 512],\n",
    "        act_fn_name=\"relu\",\n",
    "        block_name=\"ResNetBlock\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            num_classes - Number of classification outputs (10 for CIFAR10)\n",
    "            num_blocks - List with the number of ResNet blocks to use. The first block of each group uses downsampling, except the first.\n",
    "            c_hidden - List with the hidden dimensionalities in the different blocks. Usually multiplied by 2 the deeper we go.\n",
    "            act_fn_name - Name of the activation function to use, looked up in \"act_fn_by_name\"\n",
    "            block_name - Name of the ResNet block, looked up in \"resnet_blocks_by_name\"\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert block_name in resnet_blocks_by_name\n",
    "        self.hparams = SimpleNamespace(\n",
    "            num_classes=num_classes,\n",
    "            c_hidden=c_hidden,\n",
    "            num_blocks=num_blocks,\n",
    "            act_fn_name=act_fn_name,\n",
    "            act_fn=act_fn_by_name[act_fn_name],\n",
    "            block_class=resnet_blocks_by_name[block_name],\n",
    "        )\n",
    "        self._create_network()\n",
    "        self._init_params()\n",
    "\n",
    "    def _create_network(self):\n",
    "        c_hidden = self.hparams.c_hidden\n",
    "\n",
    "        # A first convolution on the original image to scale up the channel size\n",
    "        if self.hparams.block_class == PreActResNetBlock:  # => Don't apply non-linearity on output\n",
    "            self.input_net = nn.Sequential(nn.Conv2d(3, c_hidden[0], kernel_size=3, padding=1, bias=False))\n",
    "        else:\n",
    "            self.input_net = nn.Sequential(\n",
    "                nn.Conv2d(3, c_hidden[0], kernel_size=3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(c_hidden[0]),\n",
    "                self.hparams.act_fn(),\n",
    "            )\n",
    "\n",
    "        # Creating the ResNet blocks\n",
    "        blocks = []\n",
    "        for block_idx, block_count in enumerate(self.hparams.num_blocks):\n",
    "            for bc in range(block_count):\n",
    "                # Subsample the first block of each group, except the very first one.\n",
    "                subsample = bc == 0 and block_idx > 0\n",
    "                blocks.append(\n",
    "                    self.hparams.block_class(\n",
    "                        c_in=c_hidden[block_idx if not subsample else (block_idx - 1)],\n",
    "                        act_fn=self.hparams.act_fn,\n",
    "                        subsample=subsample,\n",
    "                        c_out=c_hidden[block_idx],\n",
    "                    )\n",
    "                )\n",
    "        self.blocks = nn.Sequential(*blocks)\n",
    "\n",
    "        # Mapping to classification output\n",
    "        self.output_net = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(), nn.Linear(c_hidden[-1], self.hparams.num_classes)\n",
    "        )\n",
    "\n",
    "    def _init_params(self):\n",
    "        # Based on our discussion in Tutorial 4, we should initialize the convolutions according to the activation function\n",
    "        # Fan-out focuses on the gradient distribution, and is commonly used in ResNets\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=self.hparams.act_fn_name)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_net(x)\n",
    "        x = self.blocks(x)\n",
    "        x = self.output_net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3366959818.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [3]\u001b[0;36m\u001b[0m\n\u001b[0;31m    self.backbone = getattr(models, backbone)(num_classes=self.trainer.datamodule.)\u001b[0m\n\u001b[0m                                                                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Define the LightningModule\n",
    "class ERMModule(pl.LightningModule):\n",
    "    def __init__(self, backbone='preactresnet18'):\n",
    "        super().__init__()\n",
    "        self.backbone = \n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.accuracy = Accuracy()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss_fn(logits, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss_fn(logits, y)\n",
    "        acc = self.accuracy(logits, y)\n",
    "        self.log('val_loss', loss)\n",
    "        self.log('val_acc', acc, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.SGD(self.parameters(), lr=0.1, weight_decay=2e-4)\n",
    "        lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[30, 60], gamma=0.1)\n",
    "        return [optimizer], [lr_scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the LightningDataModule\n",
    "data_module = ImbalancedSUNDataModule(data_dir='path/to/data')\n",
    "\n",
    "# Initialize the Lightning trainer\n",
    "trainer = pl.Trainer(max_epochs=90)\n",
    "\n",
    "# Train the model\n",
    "model = ImbalancedSUNModel()\n",
    "trainer.fit(model, datamodule=data_module)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch torchvision\n",
    "%pip install jupyter ipywidgets\n",
    "%pip install \"pytorch-lightning <1.9\" lightning-flash[image]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "import flash\n",
    "from flash.core.classification import LabelsOutput\n",
    "from flash.core.data.utils import download_data\n",
    "from flash.image import ImageClassificationData, ImageClassifier\n",
    "\n",
    "from flash.core.data.transforms import ApplyToKeys\n",
    "from flash.core.data.io.input_transform import InputTransform\n",
    "from dataclasses import dataclass\n",
    "from torch.utils.data import Subset\n",
    "from torchvision import datasets, transforms\n",
    "from random import shuffle\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from torchmetrics import Recall, F1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset SUN397\n",
      "    Number of datapoints: 108754\n",
      "    Root location: ./data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80a014ece5834890b98407c63b9debd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5eaa84f4b7f49efaa705742419a5234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/397 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# set the random seeds.\n",
    "seed_everything(42)\n",
    "\n",
    "# 1. Download and organize the data\n",
    "# download_data(\"https://pl-flash-data.s3.amazonaws.com/hymenoptera_data.zip\", \"data/\")\n",
    "\n",
    "ds = datasets.SUN397(\n",
    "    root='./data', \n",
    ")\n",
    "print(ds)\n",
    "\n",
    "s = [[] for _ in ds.classes]\n",
    "for i, label in tqdm(enumerate(ds._labels)):\n",
    "    s[label].append(i)\n",
    "\n",
    "idxs = {\"train\": [], \"val\": [], \"test\": []}\n",
    "for idx_list in tqdm(s):\n",
    "    shuffle(idx_list)\n",
    "    idxs[\"val\"].extend(idx_list[:10])\n",
    "    idxs[\"test\"].extend(idx_list[10:10 + 40])\n",
    "    idxs[\"train\"].extend(idx_list[50:])\n",
    "# print(idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ImageClassificationInputTransform(InputTransform):\n",
    "    image_size: Tuple[int, int] = (32, 32)\n",
    "    # mean: Union[float, Tuple[float, float, float]] = (0.485, 0.456, 0.406)\n",
    "    # std: Union[float, Tuple[float, float, float]] = (0.229, 0.224, 0.225)\n",
    "\n",
    "    def per_sample_transform(self):\n",
    "        return transforms.Compose(\n",
    "            [\n",
    "                ApplyToKeys(\n",
    "                    \"input\",\n",
    "                    transforms.Compose(\n",
    "                        [\n",
    "                            transforms.ToTensor(), \n",
    "                            # transforms.Normalize(self.mean, self.std)\n",
    "                        ]\n",
    "                    ),\n",
    "                ),\n",
    "                ApplyToKeys(\"target\", torch.as_tensor),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def train_per_sample_transform(self):\n",
    "        return transforms.Compose(\n",
    "            [\n",
    "                ApplyToKeys(\n",
    "                    \"input\",\n",
    "                    transforms.Compose(\n",
    "                        [\n",
    "                            transforms.RandomResizedCrop(self.image_size),\n",
    "                            transforms.RandomHorizontalFlip(),\n",
    "                            transforms.ToTensor(),\n",
    "                            # transforms.Normalize(self.mean, self.std),\n",
    "                        ]\n",
    "                    ),\n",
    "                ),\n",
    "                ApplyToKeys(\"target\", torch.as_tensor),\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/DATA/souraviai/miniconda3/envs/tmp/lib/python3.8/site-packages/pytorch_lightning/utilities/apply_func.py:31: LightningDeprecationWarning: `pytorch_lightning.utilities.apply_func.apply_to_collection` has been deprecated in v1.8.0 and will be removed in v1.10.0. Please use `lightning_utilities.core.apply_func.apply_to_collection` instead.\n",
      "  rank_zero_deprecation(\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "Required dependencies not available. Please run: pip install 'lightning-flash[image]'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/DATA1/souraviai/M2m/nb.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.50.20.10/DATA1/souraviai/M2m/nb.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m datamodule \u001b[39m=\u001b[39m ImageClassificationData\u001b[39m.\u001b[39mfrom_datasets(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.50.20.10/DATA1/souraviai/M2m/nb.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     train_dataset\u001b[39m=\u001b[39mSubset(ds, idxs[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m]),\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.50.20.10/DATA1/souraviai/M2m/nb.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     val_dataset\u001b[39m=\u001b[39mSubset(ds, idxs[\u001b[39m\"\u001b[39m\u001b[39mval\u001b[39m\u001b[39m\"\u001b[39m]),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.50.20.10/DATA1/souraviai/M2m/nb.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     transform\u001b[39m=\u001b[39mImageClassificationInputTransform(),\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.50.20.10/DATA1/souraviai/M2m/nb.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m )\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.50.20.10/DATA1/souraviai/M2m/nb.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# print(datamodule.num_classes)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.50.20.10/DATA1/souraviai/M2m/nb.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.50.20.10/DATA1/souraviai/M2m/nb.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# 2. Build the model using desired Task\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.50.20.10/DATA1/souraviai/M2m/nb.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m model \u001b[39m=\u001b[39m ImageClassifier(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.50.20.10/DATA1/souraviai/M2m/nb.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     backbone\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mresnet18\u001b[39;49m\u001b[39m\"\u001b[39;49m, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.50.20.10/DATA1/souraviai/M2m/nb.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     num_classes\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(s), \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.50.20.10/DATA1/souraviai/M2m/nb.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     pretrained\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.50.20.10/DATA1/souraviai/M2m/nb.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39m# metrics=F1Score(task='multiclass', num_classes=len(s)),\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.50.20.10/DATA1/souraviai/M2m/nb.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.50.20.10/DATA1/souraviai/M2m/nb.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mprint\u001b[39m(model)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.50.20.10/DATA1/souraviai/M2m/nb.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# 3. Create the trainer (run one epoch for demo)\u001b[39;00m\n",
      "File \u001b[0;32m/DATA/souraviai/miniconda3/envs/tmp/lib/python3.8/site-packages/flash/core/utilities/imports.py:164\u001b[0m, in \u001b[0;36mrequires.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    163\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 164\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mModuleNotFoundError\u001b[39;00m(\n\u001b[1;32m    165\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRequired dependencies not available. Please run: pip install \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(modules)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    166\u001b[0m     )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: Required dependencies not available. Please run: pip install 'lightning-flash[image]'"
     ]
    }
   ],
   "source": [
    "datamodule = ImageClassificationData.from_datasets(\n",
    "    train_dataset=Subset(ds, idxs[\"train\"]),\n",
    "    val_dataset=Subset(ds, idxs[\"val\"]),\n",
    "    test_dataset=Subset(ds, idxs[\"test\"]),\n",
    "    batch_size=128,\n",
    "    transform=ImageClassificationInputTransform(),\n",
    ")\n",
    "# print(datamodule.num_classes)\n",
    "\n",
    "# 2. Build the model using desired Task\n",
    "model = ImageClassifier(\n",
    "    backbone=\"resnet18\", \n",
    "    num_classes=len(s), \n",
    "    pretrained=False,\n",
    "    # metrics=F1Score(task='multiclass', num_classes=len(s)),\n",
    ")\n",
    "print(model)\n",
    "# 3. Create the trainer (run one epoch for demo)\n",
    "trainer = flash.Trainer(max_epochs=1, accelerator=\"gpu\", devices=torch.cuda.device_count())\n",
    "print(trainer)\n",
    "# 4. Train the model\n",
    "trainer.fit(model, datamodule=datamodule)\n",
    "\n",
    "# 5. Save the model!\n",
    "trainer.save_checkpoint(\"test/image_classification_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lightning-bolts==0.6.0.post1\n",
      "lightning-flash==0.8.1.post0\n",
      "lightning-utilities==0.8.0\n",
      "pytorch-lightning==1.8.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip freeze | grep lightning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.metrics import geometric_mean_score, sensitivity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "y = np.random.randint(10, size=500)\n",
    "pred = np.random.randint(10, size=500)\n",
    "# print(y)\n",
    "# print(pred)\n",
    "tmp1 = sensitivity_score(y, pred, average=None)\n",
    "print(tmp1)\n",
    "tmp2 = geometric_mean_score(y, pred)\n",
    "print(tmp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = np.load(\"./logs/Imbalance_S320_M2m_L0.5_W160_E0.1_I10_svhn_R60_resnet32_G0.9_B0.999/classwise_acc.npy\")\n",
    "r =1 \n",
    "for i in acc**(1/len(acc)):\n",
    "    r *= i\n",
    "\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from utils import get_mean_and_std\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data = datasets.SUN397(\n",
    "    root='./data', \n",
    "    # split='train',\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "# dataloader = DataLoader(\n",
    "#     data, \n",
    "#     batch_size=1, \n",
    "#     # shuffle=True, \n",
    "#     num_workers=2\n",
    "# )\n",
    "\n",
    "# m, s = get_mean_and_std(data)\n",
    "# print(m)\n",
    "# print(s)\n",
    "# data = data.data / 255 # data is numpy array\n",
    "\n",
    "# print(np.round(data.mean(axis = (0,2,3)), 4))\n",
    "# print(np.round(data.std(axis = (0,2,3)), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUN397\n"
     ]
    }
   ],
   "source": [
    "from fuzzywuzzy import process\n",
    "print(process.extractOne('sun37'.lower(), datasets.__all__, lambda x: x.lower())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPU cores: 24\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "print(\"Number of CPU cores:\", num_cores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "M2m",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
